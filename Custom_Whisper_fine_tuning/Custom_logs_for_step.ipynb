{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shri: Beginning of the custom fine tuning of Whisper for Indian Penal Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of the important constant for preprocessign of the dataset\n",
    "Certainly! I'd be happy to explain these constants in detail and discuss their importance in the context of audio processing and the Whisper model.\n",
    "\n",
    "\n",
    "\n",
    "Let's break down each constant:\n",
    "\n",
    "1. SAMPLE_RATE = 16000\n",
    "\n",
    "   This constant defines the number of samples per second in the audio signal. \n",
    "   \n",
    "   - Importance: The sample rate determines the highest frequency that can be represented in the digital audio signal (Nyquist frequency = sample rate / 2).\n",
    "   - Why 16000 Hz: This specific value (16 kHz) is chosen because it's the sample rate used by the Whisper model. Whisper was trained on audio resampled to 16 kHz, so using this sample rate ensures compatibility with the model.\n",
    "   - Effect: A 16 kHz sample rate can represent frequencies up to 8 kHz, which covers most of the frequency range important for speech recognition.\n",
    "\n",
    "2. N_FFT = 400\n",
    "\n",
    "   This constant defines the number of points used in each Fast Fourier Transform (FFT) window.\n",
    "   \n",
    "   - Importance: The FFT size affects the frequency resolution of the spectrogram. A larger FFT size gives finer frequency resolution but poorer time resolution.\n",
    "   - Why 400: This value provides a good balance between frequency and time resolution for speech audio at 16 kHz. It corresponds to a 25 ms window (400/16000 = 0.025 s), which is commonly used in speech processing as it's roughly the duration of a phoneme.\n",
    "   - Effect: This setting allows the capture of relevant spectral details in speech while maintaining good temporal resolution.\n",
    "\n",
    "3. N_MELS = 80\n",
    "\n",
    "   This constant defines the number of Mel filter banks to use in the mel spectrogram.\n",
    "   \n",
    "   - Importance: Mel spectrograms are a representation of audio that's more closely aligned with human auditory perception. The number of mel bands determines the resolution of this representation.\n",
    "   - Why 80: This is the number of mel bands used by the Whisper model. It provides a good balance between capturing relevant spectral information and reducing dimensionality compared to the full spectrogram.\n",
    "   - Effect: Using 80 mel bands allows the creation of input features that match what Whisper expects, ensuring compatibility with the pre-trained model.\n",
    "\n",
    "4. N_MFCC = 40\n",
    "\n",
    "   This constant defines the number of Mel-frequency cepstral coefficients (MFCCs) to compute.\n",
    "   \n",
    "   - Importance: MFCCs are another representation of audio that capture the overall shape of the spectral envelope in a compact form. They're often used in speech and audio processing tasks.\n",
    "   - Why 40: This is a commonly used number of coefficients in speech recognition tasks. It provides a good balance between capturing relevant information and keeping the feature set compact.\n",
    "   - Effect: While Whisper primarily uses mel spectrograms, including MFCCs can provide additional features that might be useful in the fine-tuning process or for other audio analysis tasks.\n",
    "\n",
    "These constants are crucial because they define the properties of the audio processing pipeline. Matching these values to those used in the original Whisper model (especially SAMPLE_RATE, N_FFT, and N_MELS) ensures that the audio features you generate will be compatible with the model's expected input. \n",
    "\n",
    "If you were to change these values significantly, it could lead to a mismatch between your input features and what the model expects, potentially degrading performance. However, small adjustments might be acceptable or even beneficial depending on your specific use case and the characteristics of your audio data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for performing fine tuning Whisper model\n",
    "    1. Load the pre-trained Whisper model\n",
    "    2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Very good link for Speech BRain: Where we can tokenize the text using sentencepiece and form our own tokenizer from the transcript provided: https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing#scrollTo=Ik9hoxBUG03u\n",
    "\n",
    "2. Also training a small LM model along with beam search for greater accuracy\n",
    "\n",
    "3. Tutorial sfor SpeechBrain: https://speechbrain.github.io/tutorial_basics.html\n",
    "\n",
    "4. Nvidia Speech2Text: https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition.html#speech-recognition"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
